{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Python script to scrape The Guardian Website for news articles based on protest\n",
    "#The code can be modified to get articles related to any specific topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "r = urllib.request.urlopen('https://www.theguardian.com/world/protest').read()\n",
    "soup = BeautifulSoup(r,'lxml')\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n"
     ]
    }
   ],
   "source": [
    "#get all hyperlinks from parent page\n",
    "links=[]\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 25 171\n"
     ]
    }
   ],
   "source": [
    "links=[links[i] for i in range(len(links)) if re.search(r'http',str(links[i]))!=None]\n",
    "# divide links based on protest and non-protest\n",
    "# since a lot of other links (such as advertisements) also get included\n",
    "protest_links=[links[i] for i in range(len(links)) if re.search(r'protest',str(links[i]))!=None]\n",
    "non_protest=list(set(links).difference(set(protest_links)))\n",
    "print(len(links),len(protest_links),len(non_protest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.theguardian.com/world/protest?page=4',\n",
       " 'https://www.theguardian.com/world/protest?page=2',\n",
       " 'https://www.theguardian.com/world/protest?page=291',\n",
       " 'https://www.theguardian.com/world/protest?page=3',\n",
       " 'https://www.theguardian.com/world/protest?page=5',\n",
       " 'https://www.theguardian.com/world/protest?page=6',\n",
       " 'https://www.theguardian.com/world/protest?page=7',\n",
       " 'https://www.theguardian.com/world/protest?page=8',\n",
       " 'https://www.theguardian.com/world/protest?page=9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get page links from various pages 1,2,...300 etc. which can be used to access\n",
    "#other pages on the same topic ie pages 2,3,4 etc of the search result\n",
    "\n",
    "page_links=[protest_links[i] for i in range(len(protest_links)) if re.search(r'page=',str(protest_links[i]))!=None]\n",
    "protest_links=list(set(protest_links).difference(set(page_links)))\n",
    "print(len(page_links),len(protest_links))\n",
    "\n",
    "\n",
    "page_links=list(set(page_links))\n",
    "for i in range(5,10):    #range of i can be varied based on the number of results required\n",
    "    page_links.append('https://www.theguardian.com/world/protest?page='+str(i))\n",
    "page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to get text content from link\n",
    "def getcontent(link):\n",
    "    r = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(r,'lxml')\n",
    "    soup.prettify()\n",
    "    par=''\n",
    "    for para in soup.find_all('p'):\n",
    "        par=par+para.get_text()+' '\n",
    "    return par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to get child links from parent page\n",
    "def getchildlinks(link):\n",
    "    r = urllib.request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    links=[]\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    print(len(links))\n",
    "    links=[links[i] for i in range(len(links)) if re.search(r'http',str(links[i]))!=None]\n",
    "    protest_links=[links[i] for i in range(len(links)) if re.search(r'protest',str(links[i]))!=None]\n",
    "    non_protest=list(set(links).difference(set(protest_links)))\n",
    "    print(len(links),len(protest_links),len(non_protest))\n",
    "    \n",
    "    page_links=[protest_links[i] for i in range(len(protest_links)) if re.search(r'page=',str(protest_links[i]))!=None]\n",
    "    protest_links=list(set(protest_links).difference(set(page_links)))\n",
    "    print(len(page_links),len(protest_links))\n",
    "    return protest_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruchita\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Ruchita\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n",
      "272 43 161\n",
      "5 19\n",
      "290\n",
      "283 64 158\n",
      "5 35\n",
      "287\n",
      "280 45 166\n",
      "5 24\n",
      "286\n",
      "279 50 161\n",
      "5 26\n",
      "279\n",
      "272 26 168\n",
      "5 11\n",
      "279\n",
      "272 39 163\n",
      "5 17\n",
      "243\n",
      "236 9 154\n",
      "4 4\n",
      "292\n",
      "285 54 163\n",
      "4 31\n",
      "295\n",
      "288 67 158\n",
      "5 39\n",
      "216\n"
     ]
    }
   ],
   "source": [
    "page_links=list(set(page_links))\n",
    "p_links=[getchildlinks(page_links[i]) for i in range(len(page_links))] #get child links from various pages related to protest\n",
    "for l in p_links:\n",
    "\n",
    "    protest_links.extend(l)\n",
    "print(len(protest_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc='C:/Users/Ruchita/Desktop/document classification/protest_data' #specify directory\n",
    "i=0\n",
    "#Write data to file\n",
    "for p_link in protest_links:\n",
    "    data=getcontent(p_link)\n",
    "    if len(data)>0:\n",
    "        file=open(loc+'/'+str(i)+'.txt','w',encoding = 'utf-8')\n",
    "        file.write(data)\n",
    "        file.close()\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
